TODO
1. RMSE of old and new dataframes (submissions.csv)
    - need split existing dataset into test set and training set
    - only have around 910k entries
    - afraid of overfitting
2. year vs. rating
    - # no-year = 194
    - min year = 1919
    - max year = 2000 
3. add the min-error submisson.csv as training set?
    - add submission.csv+predictions.csv on ratings.csv to generate ratings_comb.csv
    - retrain whole thing
    - compare new submission.csv with old one
    - comb: on; direct=off
        - we see that std gets slightly larger (~0.02)
        - score slightly drops (0.004)
- comb: on; direct=off (direct on == comb off; YOU IDIOT)
>>> s_old.describe()
                 Id        Rating
count  90019.000000  90019.000000
mean   45010.000000      3.612113
std    25986.391278      0.773601
min        1.000000      1.000000
25%    22505.500000      3.147937
50%    45010.000000      3.706087
75%    67514.500000      4.172045
max    90019.000000      5.000000
>>> s_new.describe()
                 Id        Rating
count  90019.000000  90019.000000
mean   45010.000000      3.610404
std    25986.391278      0.799348
min        1.000000      1.000000
25%    22505.500000      3.137494
50%    45010.000000      3.718440
75%    67514.500000      4.194488
max    90019.000000      5.000000



N=5, 0.88071
N=10, 0.85458

// TEST different N
the larger N, the smaller std, and higher the score
>>> n5.describe()
                 Id        Rating
count  90019.000000  90019.000000
mean   45010.000000      3.614949
std    25986.391278      0.831121
min        1.000000     -0.082934
25%    22505.500000      3.111181
50%    45010.000000      3.712158
75%    67514.500000      4.219291
max    90019.000000      5.935729
>>> n7.describe()
                 Id        Rating
count  90019.000000  90019.000000
mean   45010.000000      3.614587
std    25986.391278      0.802211
min        1.000000     -0.176396
25%    22505.500000      3.133383
50%    45010.000000      3.709895
75%    67514.500000      4.195756
max    90019.000000      5.863453
>>> n10.describe()
                 Id        Rating
count  90019.000000  90019.000000
mean   45010.000000      3.612602
std    25986.391278      0.778147
min        1.000000     -0.175341
25%    22505.500000      3.147937
50%    45010.000000      3.706087
75%    67514.500000      4.172045
max    90019.000000      5.893984


''' 
# First observations
1. anormaly data
    - such as age=1 user, may need to treat it differently
2. gender
    - how to encode gender into number?
3. recall from lec that latest movies have higher ratings
    - also recall that item-item CF is more reliable than user-user CF
4. if each user rated each movie, then we should have 6040 * 3706 = 22384240 ratings
    - we only have 910190 ratings
    - very sparse

# Ideas
1. CF+Latent Model
    - different users have different standards, need to normalize star ratings
2. Dealing with missing entries
3. Dealing with anomaly entries
4. process movie names
    - such as use TFIDF
    - maybe add a important-word-score in the ratings-df
5. Dealing with cold-start problem
    - for existing user, use content-based approach
6. Try different N, find albow one?
7. write own RSME evaluation

# ???
    #   2. negative sim score
    #       - https://stats.stackexchange.com/questions/198810/interpreting-negative-cosine-similarity
'''


runtime
real	47m55.372s
user	76m11.624s
sys	2m45.654s


# After first version
1. tried different N
2. make score > 5 be 5
   make score < 0 be 0
   investigate score between 0 and 1
   after observing the ratings.csv, keep min=1, max=5
3. tried using round()
    - i thought the rating is only integer, so i round each num
    - but the acc dropped from 0.85 to 0.9
4. tried selectively rounding()
    - no good
5. tried different formula for N
    - N=15 is opt. but might be a result of overfitting
    - using N=min(20, 0.2L)
6. tried dvi
    - for exception: increased 0.05%
    - for all: bad, very bad, drop 5%

