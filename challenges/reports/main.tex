%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% ************************************************************************** %%
%% *                                Settings                                * %%
%% ************************************************************************** %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\documentclass{ece}
\loadglsentries{gls}
\usepackage{tabto}
\usepackage{tabularx}
\glsaddall
\addbibresource{reference}
\usepackage[flushleft]{threeparttable}
\usepackage[autostyle=false, style=english]{csquotes}
\MakeOuterQuote{"}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Make sure the following block contains the correct information               %
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\reporttitle{Netflix Challenge: Movie Rating Prediction}
\employername{CSE-2525 Data Mining}
\employerstreetaddress{Thomas Abeel, Gosia Migut}
\authorname{Yanqing Wu}
\studentnumber{5142571}
\userid{yanqingwutudelft}
\program{Exchanged Computer Engineering}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% end of information block...                                                  %
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{document}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% ************************************************************************** %%
%% *                               Title Page                               * %%
%% ************************************************************************** %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\maketitle

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% ************************************************************************** %%
%% *                           Table of Contents                            * %%
%% ************************************************************************** %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\tableofcontents

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% ************************************************************************** %%
%% *                             List of Tables                             * %%
%% ************************************************************************** %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\listoftables

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% ************************************************************************** %%
%% *                                  Body                                  * %%
%% ************************************************************************** %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\body

\section{Introduction}

The report, entitled ``Netflix Challenge: Movie Rating Prediction'', is prepared as my Challenge report for the course CSE2525-Data Mining at the Technische Universiteit Delft.
The purpose of this report is to develop a recommendation system for predicting movie ratings.
The goal of the recommendation system is to achieve Root Mean Square Error (RMSE) as small as possible on an unseen dataset.

\subsection{Netflix Datasets}

\begin{table}[ht!]
    \caption[The Basic Information of Provided Data Sets]{The Basic Information of Provided Data Sets}
    \label{tab:table-1}
    \centering
    \begin{tabular*}{\textwidth}{@{\extracolsep{\fill}}lllllS[table-format=5.2]}	
    	% l=left-justified column, c=centered column, r=right-justified column
        % 4.4 means 4 digits before and after the decimal point	
        \hline	
        \textbf{Dataset} 
        	& \multicolumn{1}{c}{\textbf{Features}} & \multicolumn{1}{c}{\textbf{Mean}} 
            & \multicolumn{1}{c}{\textbf{Std}} & \multicolumn{1}{c}{\textbf{Min \& Max}} \\	
        \hline	
        % ~ gives space	
        users
            & gender
            & 0.72
            & 0.45
        	& \begin{tabular}{@{}l@{}} 0.00 \\ 1.00 \end{tabular}
\\
        	& age
        	& 30.64
        	& 12.90
        	& \begin{tabular}{@{}l@{}} 1.00 \\ 56.00 \end{tabular}
\\
        	& profession
        	& 8.15
        	& 6.33
        	& \begin{tabular}{@{}l@{}} 0.00 \\ 20.00 \end{tabular}
\\      movies
        	& year
        	& 1985.81
        	& 16.91
        	& \begin{tabular}{@{}l@{}} 1919.00 \\ 2000.00 \end{tabular}
\\
			& title (string)
			& -
			& -
			& -
\\		ratings
			& rating
			& 3.58
			& 1.12
			& \begin{tabular}{@{}l@{}} 1.00 \\ 5.00 \end{tabular}
\\      \hline	
   \end{tabular*}
   \begin{tablenotes}
     \item In users - `gender', `0' and `1' indicates female users and male users, respectively;
     \item In movies - `year', only non-zero entries are considered.
   \end{tablenotes}
\end{table}

Three datasets are provided for training, as described in \Cref{tab:table-1}.
There is a total of 910,190 ratings, which were given by 6,040 users and 3,706 movies.
The rest `predictions.csv' file is used for final testing, which contains only `userID` and `movieID' for each entry.
There is a total of 90,019 entries in `predictions.csv' to be predicted and the result is put on Kaggle to determine RMSE score.


\section{Methodology}

Data interpretation was first conducted at the beginning of the Challenge, as demonstrated in \Cref{tab:table-1}.
After examining the data and reviewing the course materials, the first version of the recommendation algorithm was decided as Collaborative Filtering (CF).
More specifically, Item-Item CF was selected as the first attempt to the Challenge.
Item-Item CF is more reliable than User-User CF in practice \cite{mmds}, because items are much less dynamic than users.

\subsection{Item-Item Collaborative Filtering}
\label{item-item-cf}

Collaborative filtering is an approach of ``making automatic predictions (filtering) about the interest of a user by collecting preferences information from many users (collaborating)'' \cite{wiki_cf}.
There are usually two approaches for collaborative filtering (CF), namely Item-Item CF and User-User CF.
User-User CF is generally more difficult to scale than Item-Item CF due to the dynamic nature of users, whereas items usually remains constant.
Hence, Item-item CF was selected.

\subsubsection{Core Equation}

\begin{equation}
	\begin{aligned}\label{eq:1}
& r_{xi} = b_{xi} + \frac{\sum\limits_{j\in N(i;x)} S_{ij} \cdot (r_{xj} - b_{xj}) }{\sum\limits_{j\in N(i;x)} S_{ij}}  \\
& b_{xi} = \mu + b_x + b_i
	\end{aligned}
\end{equation}

Equation \eqref{eq:1} shows the core idea of the algorithm \cite{mmds_org}.
Term $r_{xi}$ represents the rating of user $x$ on movie $i$.
Term $s_{ij}$ is the similarity of movie $i$ and movie $j$, which is measured by centered cosine similarity.
Term $b_{xi}$ is the baseline estimator for $r_{xi}$, where $\mu$ is the average ratings of all movies, $b_x$ is the rating deviation of user $x$ and $b_i$ is the rating deviation of movie $i$.

\subsubsection{Nearest Neighbors}
\label{knn}
For each prediction, the top similar movies to movie $i$ need to be determined, so that a more reasonable prediction can be made.
The top similar movies to movie $i$ is known as the \textit{nearest neighbors} of item $i$.
A parameter $N$ is used to denote the amount of nearest neighbors in the following context.
Additionally, Jaccard similarity is abandoned in determining similarity since it only considers common movies rather than ratings.
Moreover, Cosine similarity has a more informed interpretation than Jaccard similarity; however, it fails to generalize among users since different users have different standards.
Pearson correlation (a.k.a centered cosine similarity) preserves the advantage of cosine similarity and removes the bias views of users via subtracting row mean for each movie.

An upper triangular matrix of similarity score is built to fulfill all aforementioned points.

\section{Results}

This section shows the Root Mean Square Error (RMSE) of the Item-Item Collaborative Filtering system on Kaggle test set.
In addition to $N$, some extra options and parameters are introduced to improve RMSE:
\textit{Cap.},
$\epsilon$,
$L_x$ and Local Deviation.

These are explained in the following sections, along with their test results.

\subsection{Local \textit{Cap.}}

\begin{table}[!htpb]
    \caption[N and \textit{Cap.} vs. RMSE]{N and \textit{Cap.} vs. RMSE}
    \label{tab:table-2}
    \centering
\begin{tabular}{|l|l|l|}
\hline
$N$  & \textit{Cap.} & RMSE    \\ \hline
5  & off  & 0.88071 \\ \hline
7  & off  & 0.86398 \\ \hline
10 & off  & 0.85458 \\ \hline
10 & on   & 0.85397 \\ \hline
12 & on   & 0.85144 \\ \hline
15 & on   & 0.84996 \\ \hline
\end{tabular}
\end{table}

\Cref{tab:table-2} shows the result of the recommendation system (described in \ref{item-item-cf}) on Kaggle test set.
As mentioned in \ref{knn}, $N$ is the amount of nearest neighbors.

After noticing some negative ratings and nearly 6.0 ratings, option \textit{Cap.} is introduced.
The \textit{Cap.} option sets ratings that below 1.0 to 1.0 and ratings that above 5.0 to 5.0.

The maximum static $N$ is set to 15, because the minimum amount of user $x$ rated movies is 15.

\subsection{Selectively Rounding}

\begin{table}[h]
    \caption[Selectively Rounding vs. RMSE]{Selectively Rounding vs. RMSE (\textit{Cap.} = on)}
    \label{tab:table-3}
    \centering
\begin{tabular}{|l|l|l|}
\hline
$N$  & $\epsilon$ & RMSE    \\ \hline
10 & 0.1     & 0.85458 \\ \hline
10 & 0.05    & 0.85409 \\ \hline
10 & 0.02    & 0.85398 \\ \hline
10 & 0.01    & 0.85398 \\ \hline
10 & 0       & 0.85397 \\ \hline
\end{tabular}
\end{table}

\Cref{tab:table-3} shows an attempt to use rounding to improve RMSE. 
Hyper-parameter $\epsilon$ represents the range of rounding. For instance, $\epsilon=0.05$ rounds any ratings in the range of $[3.95, 4.05]$ to a 4.0.

\subsection{Dynamic $N$}

\begin{table}[h]
    \caption[Dynamic N vs. RMSE]{Dynamic N vs. RMSE (\textit{Cap.} = on)}
    \label{tab:table-4}
    \centering
\begin{tabular}{|l|l|}
\hline
$N$            & RMSE    \\ \hline
$L_x$                    & 0.87736 \\ \hline
min(round(0.1$L_x$), 15) & 0.85761 \\ \hline
min(round(0.2$L_x$), 20) & 0.85280 \\ \hline
min(round(0.3$L_x$), 30) & 0.85307 \\ \hline
max(round(0.1$L_x$), 5)  & 0.85754 \\ \hline
max(round(0.2$L_x$), 5)  & 0.86148 \\ \hline
\end{tabular}
\end{table}

\Cref{tab:table-4} shows an attempt to use dynamic $N$ to improve RMSE.
Parameter $L_x$ is the amount of user $x$ rated movies.
Given the fact that $L_x$ ranges from 15 to 2,097, a dynamic way of choosing $N$ is introduced to avoid overfitting or underfitting.

\subsection{Local Deviation}

\begin{table}[h]
    \caption[Local Deviation vs. RMSE]{Local Deviation vs. RMSE (\textit{Cap.} = on)}
    \label{tab:table-5}
    \centering
\begin{tabular}{|l|l|l|}
\hline
$N$                        & Dvi.            & RMSE    \\ \hline
min(round(0.2$L_x$), 20) & -               & 0.85280 \\ \hline
min(round(0.2$L_x$), 20) & exception users & 0.85275 \\ \hline
min(round(0.2$L_x$), 20) & all users       & 0.90199 \\ \hline
\end{tabular}
\end{table}

\Cref{tab:table-5} shows an attempt to use local deviation to improve RMSE.
Local deviation is introduced to address cold-start problem (i.e., when there is insufficient information to make an decision).
Local deviation takes user's gender, age, profession and movie's year-of-release into consideration.

\section{Discussion}

\subsection{Discussion on The Results}

A general trend of $N$ vs. RMSE can be observed in \Cref{tab:table-2}, RMSE drops when $N$ increases.
This result is desired, since the more relative information is taken during training, the more accurate prediction will be.
Moreover, RMSE drops dramatically when $N$ is small; RMSE still drops when $N$ is above a certain threshold (10 here), but not as obvious as a small $N$.
This result makes sense, because there is a diminishing return between the amount of information and the final prediction.
Furthermore, option \textit{Cap.} improved the result due to the elimination of out-of-range ratings.

As demonstrated in \Cref{tab:table-3}, Seletively Rounding $\epsilon$ does not help minimizing RMSE.
Since the final rating is an integer, the author had a belief that rounding those almost certain ratings to their nearest integers would enhance the final result.
Nevertheless, the result did not improve.
The author concludes that there might not be as many worth-rounding ratings as expected in the datasets.

As shown in \Cref{tab:table-4}, Dynamic $N$ did not improve RMSE.
Although dynamic $N$ did not achieve a lower RMSE, the author believes that using a dynamic $N$ is more reliable than a static $N$ in terms of avoid overfitting or underfitting.

The last attempt of minimizing RMSE is using local deviation to address cold-start problem.
Before using local deviation, new users or new movies or a combination of both were given the global average rating.
After using local deviation, the RMSE improved from 0.85280 to 0.85275 while kept other parameters constant (see \Cref{tab:table-5}).

\subsection{Extra Points}

Two interesting facts were discovered when calculating local deviations.

\textbf{1}. Old movies have higher average ratings.

In contrast to what we learned in lecture that "newer movie have higher ratings".
This may be a result of different datasets.

\textbf{2}. Some users are too lazy to change default profiles.

Around 200 users (out of 3,706 users) were having $age=1$ and $profession=0$.
These users are considered as abnormal data entries during training, which are later ignored in the calculation of local deviations.

\subsection{One More Thing}

Since there is only a limited 

\section{Conclusions and Future Work}

In this report, the objective was met.
An Item-Item Collaborative Filtering recommendation system was successfully developed, achieving a minimum RMSE of 0.84991 on the Kaggle leader board.
From the analysis in the report body, it was concluded that...

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% ************************************************************************** %%
%% *                               References                               * %%
%% ************************************************************************** %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\printbibliography[heading=none]

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% ************************************************************************** %%
%% *                               Appendices                               * %%
%% ************************************************************************** %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%% appendices use section and subsection numbering
%\appendix
%
%\section{Title of First Appendix}
%\label{app:firstappx}
%Use the No Spacing style.
%
%\section{Another Appendix}
%\label{app:anotherappx}
%Again, use the no spacing style for appendices.

\end{document}
