{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Lab: Clustering using CURE\n",
    "\n",
    "Data Mining 2019/2020\n",
    "\n",
    "By Jordi Smit and Gosia Migut\n",
    "\n",
    "**WHAT** This _optional_ lab consists of several programming and insight exercises/questions. These exercises are ment to let you practice with the theory covered in: \"Mining of Massive Datasets\" by J. Leskovec, A. Rajaraman, J. D. Ullman. <br>\n",
    "\n",
    "**WHY** Practicing, both through programming and answering the insight questions, aims at deepening your knowledge and preparing you for the exam. <br>\n",
    "\n",
    "**HOW** Follow the exercises in this notebook either on your own or with a friend. Use Mattermost to disscus questions with your peers. For additional questions and feedback please consult the TA's at the assigned lab session. The answers to these exercises will not be provided.\n",
    "\n",
    "**SUMMARY**\n",
    "In the following exercises you will implement the CURE algorithm. This is a clustering algorithm designed for very large data sets that don't fit into memory. In this exercise we will simulate the limited amount of memory by dividing the data into sub batches.\n",
    "\n",
    "**Requirements**\n",
    " - Python 3.6 or higher\n",
    " - numpy\n",
    " - scipy\n",
    " - ipython\n",
    " - jupyter\n",
    " - matplotlib\n",
    " - tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install tqdm\n",
    "\n",
    "\n",
    "from tqdm import trange\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import random\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0 The problem\n",
    "K-means and Hierarchical Clustering are two very well known clustering algorithms. Both these algorithms only work if the entire data set is in the main memory, which means that there is an upper limit on the amount of data they can cluster. So if we want to go beyond this upper limit we need an algorithm that doesn't need the entire data set to be in the main memory. \n",
    "\n",
    "\n",
    "In this exercise we look at the approach of the CURE algorithm. The idea of the CURE algorithm is rather simple. We don't need the entire data set since most of the data is very similar. So we take a random sample of the data set that fits into memory and we cluster this data. We then go through the remaining data and assign it to the closest cluster.\n",
    "\n",
    "\n",
    "The CURE algorithm has the following pseudo code:\n",
    "```\n",
    "data_samples = sample_m_data_point_from_the_data_set()\n",
    "k_sample_cluster = cluster(data_samples, k)\n",
    "cure_clusters = []\n",
    "foreach cluster in k_sample_cluster:\n",
    "\tpoints = find_k_most_representive_point(cluster)\n",
    "\tcenter = find_center(points)\n",
    "\tforeach point in points:\n",
    "\t\tmove point x% towards the center\n",
    "\tadd cure_cluster(points) to cure_clusters \n",
    "\n",
    "foreach dp in unseen data:\n",
    "\tassign dp to cure_cluster of the closest representive point\n",
    "```\n",
    "\n",
    "If you want more explanation, see [this online video lecture](https://www.youtube.com/watch?v=JrOJspZ1CUw) from the authors of the book."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 Setting up\n",
    "Lets get started by creating the data structures for this problem.  We have already created a class for the *Cluster* for you. This class stores its centroid and the data points that have been assigned to it.  This class will be used for the traditional hierarchical clustering.\n",
    "You can see a summery of its class signature and its documentation using the function `help(Cluster)` or you can look at its implementation by opening the `cure_helper.py`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next let's define the CureCluster class. This class has two attributes, namely the `k_most_representative_points` and `data` (the clusters that have been assigned to it). The class is almost finished. The only thing left to do is to finish the distance function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cure_helper import Cluster\n",
    "\n",
    "#help(Cluster)\n",
    "\n",
    "class CureCluster:\n",
    "    def __init__(self, k_most_representative_points):\n",
    "        self.k_most_representative_points = k_most_representative_points\n",
    "        self.data = None\n",
    "        \n",
    "    def distance(self, cluster):\n",
    "        \"\"\"\n",
    "        Calculates the distances between the centroid of the cluster and the closest representitve point.\n",
    "\n",
    "        Parameters:\n",
    "        cluster: Cluster: The cluster with its data points we are intrested in.\n",
    "\n",
    "        Returns:\n",
    "        float: Returns the distance as a float.\n",
    "        \"\"\"\n",
    "        min_dist = sys.float_info.max\n",
    "        #Student start\n",
    "        \n",
    "        \n",
    "        #Student end\n",
    "        return min_dist\n",
    "    \n",
    "    def append(self, cluster):\n",
    "        \"\"\"\n",
    "        Adds a data point to this cluster.\n",
    "        !!!!Is statefull.!!!!\n",
    "\n",
    "        Parameters:\n",
    "        cluster: Cluster: A cluster that contains the datapoints we want to add.\n",
    "        \"\"\"\n",
    "        if self.data is None:\n",
    "            self.data = cluster\n",
    "        else:\n",
    "            self.data = self.data.merge(cluster)\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return f\"CureCluster(\\nrepresentative_points:\\n{self.k_most_representative_points},\\ndata: \\n{self.data}\\n)\\n\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next cell we import some helper functions we have already created for you: \n",
    " - `load_data`;\n",
    " - `plot_clusters`;\n",
    " - `plot_data`;\n",
    " - `plot_cure_clusters` ;\n",
    " - `hierarchical_clustering`;\n",
    " - `find_two_closest`;\n",
    " \n",
    " \n",
    "You can read there documentation using python's `help` function, as shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cure_helper import load_data\n",
    "from cure_helper import plot_clusters\n",
    "from cure_helper import plot_data\n",
    "from cure_helper import plot_cure_clusters\n",
    "from cure_helper import hierarchical_clustering\n",
    "from cure_helper import find_two_closest\n",
    "from cure_helper import find_centroid\n",
    "\n",
    "help(load_data)\n",
    "help(plot_clusters)\n",
    "help(plot_data)\n",
    "help(plot_cure_clusters)\n",
    "help(hierarchical_clustering)\n",
    "help(find_two_closest)\n",
    "help(find_centroid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 CURE\n",
    "Next, let's define the `find_k_most_representative_points` function. We'll use this function to find the $k$ most representative points in a cluster we have found using the `hierarchical_clustering` function. It is your job to find the $k$ most representative point in the data of this cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_k_most_representative_points(cluster, k):\n",
    "    \"\"\"\n",
    "    Finds the k most representative points.\n",
    "    \n",
    "    Parameters:\n",
    "    cluster: Cluster: The cluster we are intrested in.\n",
    "    k: int: The amount of representative_points.\n",
    "\n",
    "    Returns:\n",
    "    CureCluster: Returns a k x 2 matrix. Where each row contains the a representive point.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Divides each data point in the cluster into a seperate cluster.\n",
    "    datapoints_as_singleton_clusters = cluster.data_points_as_cluster()\n",
    "    # Student start\n",
    "    # Find k_clusters most representative points in the datapoints of the cluster.\n",
    "    k_most_representative_points = None\n",
    "    # Student end\n",
    "    \n",
    "    return np.array([c.centroid for c in k_most_representative_points])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part we'll combine the previously defined functions that transform a cluster, we have found using the `hierarchical_clustering` function, into a cure cluster. It's your job to find the `k_most_representative_points` and to prepare them before we create a new instance of the `CureCluster`.\n",
    "\n",
    "**Hint**: Carefully look at the pseudo code of the algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tranform_into_cure_cluster(cluster, representative_points, move_to_center_precentage):\n",
    "    \"\"\"\n",
    "    Transforms a give cluster into a cure cluster by\n",
    "    - selecting k representive points from the data assigned to this cluster.\n",
    "    - moving the k representive points towards their centroid a give precentage.\n",
    "    \n",
    "    Parameters:\n",
    "    cluster: Cluster: The cluster we want to transform.\n",
    "    representative_points: int: The amount of representative_points.\n",
    "    move_to_center_precentage: float: How much the k points should be move towards their centroid.\n",
    "\n",
    "    Returns:\n",
    "    CureCluster: Returns a new Cure Clusters with its k representive points.\n",
    "    \"\"\"\n",
    "    \n",
    "    assert 0 < move_to_center_precentage < 1, \"The value of move_to_center_precentage must be in the range (0,1)\"\n",
    "    if representative_points > len(cluster):\n",
    "        print(f\"[Warning] This cluster only has {len(cluster)} datapoints you requested {representative_points} points. Representative_points has been changed to {len(cluster)} for this cluster.\")\n",
    "        representative_points = len(cluster)\n",
    "    \n",
    "    #Student start\n",
    "    # Find the k_most_representative_points\n",
    "    k_most_representative_points = None\n",
    "    #Calc the centroid of the k_most_representative_points\n",
    "    centroid = None\n",
    "    #Move the k_most_representative_points a bit to the centroid\n",
    "    k_most_representative_points = None\n",
    "    #Student end\n",
    "    \n",
    "    return CureCluster(k_most_representative_points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next lets define the `find_cure_cluster_with_min_dist` function. It's your job to find and return the `CureCluster` in the input list that is closest to this the input `Cluster`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_cure_cluster_with_min_dist(cure_clusters, cluster):\n",
    "    \"\"\"\n",
    "    Parameters:\n",
    "    cure_clusters: List[CureCluster]: The cure clusters we want to compare against.\n",
    "    cluster: Cluster: The cluster we are intrested in.\n",
    "\n",
    "    Returns:\n",
    "    CureCluster: Returns the Cure Clusters with the minimal distance to the cluster.\n",
    "    \"\"\"\n",
    "    \n",
    "    #Student start\n",
    "    cure_cluster_with_min_dist = None\n",
    "\n",
    "    # Student end\n",
    "    return cure_cluster_with_min_dist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 Results\n",
    "\n",
    "These are the hyperparameters of the algorithm:\n",
    "\n",
    " - seed: A random seed to ensure that the random sampling returns the same result between different run;\n",
    " - sample_size: The amount of random samples we use to find the k clusters;\n",
    " - representative_points: The number of representative points we take from the k clusters;\n",
    " - n_clusters: The number of clusters we want to find.\n",
    " - move_to_center_precentage: How much the k representative points will be move towards their centroid.\n",
    " \n",
    " \n",
    " We have two sets *'data/cluster.txt'* and *'data/cluster_lines.txt'*.\n",
    " Try to find the correct hyperparameters for both sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CURE parameters\n",
    "seed = 42\n",
    "#Student start\n",
    "sample_size = 35\n",
    "representative_points = 3\n",
    "n_clusters = 3\n",
    "move_to_center_precentage = 0.2\n",
    "#Student end\n",
    "# data set 1\n",
    "file_path = \"data/cluster.txt\"\n",
    "# data set 2\n",
    "#file_path = \"data/cluster_lines.txt\"\n",
    "\n",
    "# select the correct distance measure of the current problem. If you don't know select one at random and see what happens.\n",
    "distance_measure=\"mean_sqaured_distance\"\n",
    "#distance_measure=\"closests_point\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First lets see what the data looks like. (You might want to change your distance measure in the *Cluster* class after seeing this)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_data(file_path)\n",
    "plot_clusters(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, lets sample some random points. Make sure that you have enough samples in each cluster. If this is not the case you might want to change your hyper parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(seed)\n",
    "data_sample = random.sample(data, sample_size)\n",
    "plot_clusters(data_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets assume we have a good, well distributed random sample from the data. Now lets perform traditional hierarchical clustering on this sample of the data set. The resulting clusters should be the same clusters as we visually saw in the original data sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cluster samples\n",
    "sample_clusters = hierarchical_clustering(data_sample, k=n_clusters, distance_measure=distance_measure)\n",
    "print(\"The resulting clusters of the sample data:\")\n",
    "plot_clusters(sample_clusters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets use the clusters to create $k$ cure clusters with the functions you have created. Then lets loop through all the data and lets assign each datapoint to the correct CURE cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create CURE Clusters\n",
    "cure_clusters = [tranform_into_cure_cluster(cluster, representative_points, move_to_center_precentage) for cluster in sample_clusters]\n",
    "\n",
    "# Assign remaining data to the clusters\n",
    "for dp in data:\n",
    "    cure_cluster = find_cure_cluster_with_min_dist(cure_clusters, dp)\n",
    "    cure_cluster.append(dp)\n",
    "    \n",
    "\n",
    "print(\"The resulting cluster on all the data. Whereby the dots are the data points and the diamond are the representative points of the cluster\")\n",
    "plot_cure_clusters(cure_clusters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you have implemented everything correctly and you have chosen some good hyperparameters, then your results from the CURE version should be very similar to the result of the traditional hierarchical clustering function you see below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_data_set_results = hierarchical_clustering(data, k=n_clusters, distance_measure=distance_measure)\n",
    "plot_clusters(full_data_set_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 Questions\n",
    "**What is the advantage this algorithm has over the BFR algorithm?**\n",
    "\n",
    "\n",
    "**What happens if *sample_size* hyperparameter is too high or too low?**\n",
    "\n",
    "\n",
    "**What happens if *representative_points* hyperparameter is too high or too low?**\n",
    "\n",
    "\n",
    "**What is the effect of different distance measuremets on the final result?**\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
