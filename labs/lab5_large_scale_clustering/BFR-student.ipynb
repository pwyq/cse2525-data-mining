{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Lab: Clustering using BFR\n",
    "\n",
    "Data Mining 2019/2020\n",
    "\n",
    "By Jordi Smit and Gosia Migut\n",
    "\n",
    "**WHAT** This _optional_ lab consists of several programming and insight exercises/questions. These exercises are ment to let you practice with the theory covered in: \"Mining of Massive Datasets\" by J. Leskovec, A. Rajaraman, J. D. Ullman. <br>\n",
    "\n",
    "**WHY** Practicing, both through programming and answering the insight questions, aims at deepening your knowledge and preparing you for the exam. <br>\n",
    "\n",
    "**HOW** Follow the exercises in this notebook either on your own or with a friend. Use Mattermost to disscus questions with your peers. For additional questions and feedback please consult the TA's at the assigned lab session. The answers to these exercises will not be provided.\n",
    "\n",
    "**SUMMARY**\n",
    "In the following exercises you will implement the BFR algorithm. This is a clustering algorithm designed for very large data sets that don't fit into memory. In this exercise we will simulate the limited amount of memory by dividing the data into sub batches.\n",
    "\n",
    "**Requirements**\n",
    " - Python 3.6 or higher\n",
    " - numpy\n",
    " - scipy\n",
    " - ipython\n",
    " - jupyter\n",
    " - matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "from uuid import UUID\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "import numpy as np\n",
    "import sys\n",
    "import uuid\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0 The problem\n",
    "K-means and Hierarchical Clustering are two very well known clustering algorithms. Both these algorithms only work if the entire data set is in the main memory, which means that there is an upper limit on the amount of data they can cluster. So if we want to go beyond this upper limit we need an algorithm that doesn't need the entire data set to be in the main memory. In this exercise we look at the approuch of the BFR algorithm. We will simulate the lack of memory by dividing the data in a list of lists. Whereby each sub list is a different batch that has 'supposedly' been read from disk or some other storage server.\n",
    "\n",
    "\n",
    "\n",
    "BFR works by summarizing the clusteringdata into statistical data, such as the Sum, Squared Sum and number of data points per cluster. Which means that it has to read each data point only once. The algorithm uses three sets that contain clustering summarizations:\n",
    "- **Discard Set**:\n",
    "Contains the summarizations of the data points that are *close enough* (we'll define this later on) to one of the main clusters.\n",
    "- **Compressed Set** (a.k.a mini cluster):\n",
    "Contains the summarizations of the data points that are not *close enough* to one of the main clusters but form mini clusters with other points that are not *close enough* to one of the main clusters.\n",
    "- **Retained Set**: \n",
    "Contains data points that are not *close enough* to one of the main clusters and not *close enough* to one of the mini clusters. (This are summarizations of a single datapoint).\n",
    "\n",
    "BFR uses the first chunk of data to find the $k$ main clusters and puts them into the **Discard set**. Then it loops through the remaining chunk of data. For each data point in this chunk it will check if the data point is  *close enough*. If the data point is *close enough* it will be added to the **Discard set** if not it will be added to the **Retained Set**. After we have sorted the data in this chunk we check if we can find any new mini clusters in the **Retained Set**. All the new none singleton clusters will be added the the **Compressed Set** while all the singleton clusters will stay in the **Retained Set**. Before we continue to the next chunk we have to check if we don't have to many mini clusters in **Compressed Set**. We can reduce the number of mini clusters by combining them through clustering. After we have gone through all the data we end up $k$ main cluster, $m$ mini clusters and $n$ retained data points. Because we only want $k$ clusters we need to combine all these summarizations, which can also be done using clustering.\n",
    "\n",
    "\n",
    "After we have done all this we end up with $k$ cluster summarizations, which can be used to assign future data to the closest clusters.\n",
    "\n",
    "If you want more explanation, see [this online video lecture](https://www.youtube.com/watch?v=NP1Zk8MY08k) from the authors of the book."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 Setting up\n",
    "Lets get started by creating the data structures for this problem. First of all we need to create a class for the DataPoint. This class stores the vector location and to which cluster it has been assigned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataPoint(object):\n",
    "    \"\"\"\n",
    "    A datapoint that can be clustered.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, vector):\n",
    "        self.vector = vector\n",
    "        self.cluster_id = None\n",
    "\n",
    "    def to_singleton_cluster(self):\n",
    "        \"\"\"\n",
    "        Returns:\n",
    "        Cluster: A cluster with a single data point.\n",
    "        \"\"\"\n",
    "        sum_v = self.vector\n",
    "        squared_sum = sum_v ** 2\n",
    "        n_data_points = 1\n",
    "        self.cluster_id = uuid.uuid4()\n",
    "        return BFRCluster(sum_v, squared_sum, n_data_points, set([self.cluster_id]))\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"DataPoint(vector: {self.vector}, cluster_id: {self.cluster_id})\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next lets create a class for the BFR cluster. This class must store both the statistical summarization of the data and be usable with hierarchical clustering. All the hierarchical clustering related logic has already been implemented in its parent class `Cluster`. You can read its documentation using `help(Cluster)` or see its implementation in `bfr_helper.py`.\n",
    "\n",
    "However the statistical summarization and BFR related logic must still be implemented. Now it is your job to:\n",
    " - Define the ***mean*** attribute;\n",
    " - Define the ***variance*** attribute;\n",
    " - Define the ***std*** attribute;\n",
    " - Finish the ***is_data_point_sufficiently_close*** method, used to  determine if a *DataPoint* is close enough to be added to the discard set;\n",
    " - Finish the ***mahalanobis_distance*** method, the distance measure used by the ***is_data_point_sufficiently_close*** function;\n",
    "\n",
    "We define a *DataPoint* as close enough if the $MD < 3 * std_i$ for any $i$. Where $i$ is the axis index and $MD$ is the *mahalanobis distance*.\n",
    "\n",
    "**Hints:**\n",
    " - ${\\sigma_i}^2 = \\frac{SUMSQ}{N}$  \n",
    " \n",
    " - $\\bar{x_i} = \\frac{SUM}{N}$ \n",
    " \n",
    " - $MD =\\sum_{i=1}^{N} {(\\frac{x_i - \\bar{x_i}}{\\sigma_i})^2}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bfr_helper import Cluster\n",
    "# help(Cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BFRCluster(Cluster):\n",
    "    \"\"\"\n",
    "    A sumerization of multiple data points.\n",
    "    \"\"\"\n",
    "    def __init__(self, sum_v, squared_sum, n_data_points, cluster_ids):\n",
    "        # Student start\n",
    "        mean = None\n",
    "        variance = None\n",
    "        std = None\n",
    "        # Student end\n",
    "        \n",
    "        super().__init__(sum_v, squared_sum, n_data_points, cluster_ids, mean, variance, std)\n",
    "        \n",
    "    def is_singleton(self):\n",
    "        \"\"\"\n",
    "        Returns:\n",
    "        Cluster: Return is the cluster only has a single data point..\n",
    "        \"\"\"\n",
    "        return self.n_data_points == 1\n",
    "\n",
    "    def mahalanobis_distance(self, dp):\n",
    "        \"\"\"\n",
    "        Parameters:\n",
    "        dp: DataPoint: The DataPoint we are intrested in.\n",
    "\n",
    "        Returns:\n",
    "        float: The mahalanobis distance between the centroids of this cluster and a datapoint\n",
    "        \"\"\"\n",
    "        # Student start\n",
    "        return 0\n",
    "        # Student end\n",
    "    \n",
    "    def is_data_point_sufficiently_close(self, dp):\n",
    "        \"\"\"\n",
    "        Parameters:\n",
    "        dp: DataPoint: The DataPoint we are intrested in.\n",
    "\n",
    "        Returns:\n",
    "        bool: True iff the mahalanobis distance is less than 3 times the std on atleast one axis.\n",
    "        \"\"\"\n",
    "        # Student start\n",
    "        \n",
    "        # Student end\n",
    "        return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the code below to verify that the functions were implemented correctly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "v = np.random.rand(3,2)\n",
    "cluster = BFRCluster(np.sum(v, axis=0, keepdims=True), np.sum(v ** 2, axis=0, keepdims=True), len(v), [uuid.uuid4()])\n",
    "\n",
    "#Verify that mean is implemented correctly\n",
    "assert cluster.mean.shape == (1,2)\n",
    "assert cluster.mean[0][0] == 0.420850900367068\n",
    "\n",
    "#Verify that variance is implemented correctly\n",
    "assert cluster.variance.shape == (1,2)\n",
    "assert cluster.variance[0][1] == 0.10571935835911339\n",
    "\n",
    "#Verify that std is implemented correctly\n",
    "assert cluster.std.shape == (1,2)\n",
    "assert cluster.std[0][0] == 0.23741019819501288\n",
    "\n",
    "#Verify that mahalanobis_distance is implemented correctly\n",
    "dp = DataPoint(np.random.rand(1,2))\n",
    "assert cluster.mahalanobis_distance(dp) == 3.1732638628025542\n",
    "\n",
    "inpoint = DataPoint(cluster.mean)\n",
    "outpoint = DataPoint(2 * cluster.mean)\n",
    "assert cluster.is_data_point_sufficiently_close(inpoint)\n",
    "assert not cluster.is_data_point_sufficiently_close(outpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next cell we import some helper functions we have already created for you:\n",
    " - `load_data`;\n",
    " - `hierarchical_clustering`;\n",
    "\n",
    "You can read there documentation using python's `help` function, as shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bfr_helper import hierarchical_clustering\n",
    "from bfr_helper import load_data\n",
    "\n",
    "help(hierarchical_clustering)\n",
    "help(load_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# 2 BFR\n",
    "In this section we'll use the previously defined data structures and functions to create the BFR algorithm. Lets get started by defining the *find_index_sufficiently_close_cluster* function. This function needs to return the index of a cluster that is sufficiently close. (**Hint** *we have already defined a function for this*). If no cluster is close enough it should return None."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_index_sufficiently_close_cluster(k_points, dp):\n",
    "    \"\"\"\n",
    "    Finds the index of the most representative cluster from the give k points\n",
    "\n",
    "    Parameters:\n",
    "    k_points List[Cluster]: The K clusters in the discard set.\n",
    "    dp: DataPoint: The datapoint we are intrested in.\n",
    "\n",
    "    Returns:\n",
    "    Optional[int]:Returns the index of the representive cluster. Returns None if no cluster is representative\n",
    "\n",
    "    \"\"\"\n",
    "    # Student start\n",
    "    \n",
    "    # Student end\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are the hyper parameters of the algorithm:\n",
    "\n",
    " - chunk_size: How much data can we store in a single memory scan;\n",
    " - k_clusters: The final amount of clusters we want;\n",
    " - n_discard_clusters: The number of discard cluster we'll have in the algorithm;\n",
    " - n_mini_clusters: The number of mini cluster we keep between memory scans;\n",
    " - n_new_mini_clusters: The number of new mini cluster we create per memory scan;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Algo hyperparameters\n",
    "file_path = \"data/cluster.txt\"\n",
    "chunk_size = 35\n",
    "data = load_data(file_path, chunk_size, create_data_point_func=DataPoint)\n",
    "k_clusters = 3\n",
    "n_discard_clusters = 3\n",
    "n_mini_clusters = 25\n",
    "n_new_mini_clusters = 25"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this cell we'll implement the BFR algorithm. It is your job to:\n",
    "\n",
    "- For the first chunk:\n",
    "\t - Fill the discard set using the first chunk;\n",
    "- For the remaining chunk:\n",
    "\t - Add each point that is sufficiently close to a cluster to that clusters;\n",
    "\t - Add each point that is not sufficiently close to a cluster to the retained set;\n",
    "\t - Combine each point in the retained set  that are closest to each other into mini clusters while keeping points that are not close to any other point in the retained set;\n",
    "     - After all chunk:\n",
    "\t - Combine the discard, compressed and retained set into the wanted amount of $K$ clusters.\n",
    "\n",
    "**Hints**\n",
    "\n",
    " - You can combine the clusters that are closest to each other using `hierarchical_clustering`;\n",
    " - Carefully look at the functions we have defined in the previous part. Most of the logic is already defined there;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "discard = []\n",
    "compressed = []\n",
    "retained = []\n",
    "\n",
    "for dp in data[0]:\n",
    "    # Student start\n",
    "    singleton_cluster = None\n",
    "    \n",
    "    # Student end\n",
    "    \n",
    "# Student start\n",
    "# Fill discard with k representative points\n",
    "discard = None\n",
    "# Student end\n",
    "\n",
    "\n",
    "for chunk in data[1:]:\n",
    "    for dp in chunk:\n",
    "        index_sufficiently_close_cluster = find_index_sufficiently_close_cluster(discard, dp)\n",
    "        if index_sufficiently_close_cluster is not None:\n",
    "            # Student start\n",
    "            # Replace the sufficiently_close_cluster with the merged cluster\n",
    "            pass\n",
    "            # Student end\n",
    "        else:\n",
    "            # Student start\n",
    "            # transfor datapoint into singleton cluster\n",
    "            pass           \n",
    "            # add the singleton cluster to the retrained set\n",
    "            pass\n",
    "            # Student end\n",
    "   \n",
    "    # Student start  \n",
    "    # find new mini clusters in the retained set\n",
    "    new_mini_clusters = None\n",
    "    retained = None\n",
    "    new_mini_clusters = None\n",
    "    compressed = None\n",
    "    # Student end\n",
    "\n",
    "# Combine the remaining summarization of the clusters. \n",
    "combine_summarization = discard + compressed + retained\n",
    "# Combine the summarization untill there are only k_clusters\n",
    "# Student start  \n",
    "resulting_clusters = None       \n",
    "# Student end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 Results\n",
    "And we are done! The only thing left to do is to look at the final result. Run the cell below to visualize the resulting clusters. The small dots are the data points, while the diamond are the centroids of the clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "marker_dp = \".\"\n",
    "marker_cluster = \"D\"\n",
    "k = len(resulting_clusters)\n",
    "colors = cm.rainbow(np.linspace(0,1,k))\n",
    "\n",
    "# Plot the centroids of the clusters\n",
    "for i, cluster in enumerate(resulting_clusters):\n",
    "    x = cluster.mean[:, 0]\n",
    "    y = cluster.mean[:, 1]\n",
    "    plt.scatter(x, y, marker=marker_cluster, c='k')\n",
    "\n",
    "# Plot the assigned data\n",
    "for chunk in data:\n",
    "    for dp in chunk:\n",
    "        x = dp.vector[:, 0]\n",
    "        y = dp.vector[:, 1]\n",
    "        color = None\n",
    "        for i, cluster in enumerate(resulting_clusters):\n",
    "            if cluster.contains(dp):\n",
    "                color = colors[i]\n",
    "                break\n",
    "        assert color is not None\n",
    "        plt.scatter(x, y, marker=marker_dp, c=[color])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **4.1 This algorithm has one major assumption. What is this assumptions?**\n",
    "\n",
    "*(hint it has something to do with data distribution.)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **4.2 What is the major disadvantage of this assumption?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **4.3 How many secondary memory pass do this algorithm have to make?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### **4.4 Lets say we have a dataset with 3 clusters A, B & C. What happens if the first chunk only has data from the A cluster?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
